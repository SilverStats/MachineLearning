---
title: "Prediction of Exercise Types"
author: "Steven Silverman"
date: "Practical Machine Learning"
output: html_document
---

Machine learning is an extremely powerful tool. Here, it is used to predict the manner in which study participants performed a certain exercise. Data are from Groupware@LES.

## Data Gathering and Cleaning

First, we need to load in the data sets and required packages. Please note that you must have `pml-training.csv` and `pml-testing.csv` in your working directory. We will also set the random seed to ensure reproducibility.

```{r, cache=TRUE, collapse=TRUE}
set.seed(1701)
library(caret)
library(randomForest)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
nrow(training)
ncol(training)
nrow(testing)
ncol(testing)
```

Note that with only 20 test cases and over 19000 training cases, we should be able to reduce the size of the training set to improve computation time. We will do so randomly, as follows.

```{r, cache=TRUE}
prob <- rnorm(nrow(training))
smallTraining <- training[prob > 0.5,]
```

We can also eliminate unneeded columns. A quick manual check shows that many variables have primarily missing values in both the test and training sets. We eliminate such values with `nearZeroVar`.

```{r, cache=TRUE}
zero <- nearZeroVar(testing)
smallGoodTraining <- smallTraining[,-zero]
goodTesting <- testing[,-zero]
```

Finally, the first few columns are simply identifying information such as names and timestamps and will not help in predicting test cases. As such, we disregard them.

```{r,cache=TRUE}
smallGoodTraining <- smallGoodTraining[,7:ncol(smallGoodTraining)]
goodTesting <- goodTesting[,7:ncol(goodTesting)]
```

## Model Building

To train the model, we make use of the random forest method in the `train()` function. Random forests inherently use cross-validation, as each tree in the forest is a different bootstrap, meaning that there is internal cross-validation. For more, see http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm. I did not perform any principal component analysis, but the reduction in the number of predictors with the `nearZeroVar()` function will certainly aid in computation time and the reduction in size will avoid overfitting.

(Note: the model fitting will take a few minutes.)

```{r, cache=TRUE}
fit <- train(classe ~ ., data = smallGoodTraining, method = "rf")
```

## Results

The random forest method produces great accuracy, as seen below. In addition, the out-of-bag error rate (which is an estimate of the out-of-sample error rate thanks to the bootstrapping) is quite low, at only 1.82%.

```{r, cache=TRUE}
fit
fit$finalModel
```

Below are the model's predictions for the testing data (disclosable now that the deadline has passed). Note that it actually got one wrong (a different run of the model got all correct), which is not surprising given the predicted error rate.

```{r, cache=TRUE}
prediction <- predict(fit, goodTesting)
prediction
```

## Conclusion

The model performed very well for such a quick procedure. I don't have any plots to show, since there are so many predictors and it would be tough to show them all, but the results largely speak for themselves.
